# -*- coding: utf-8 -*-
"""Data Analysis of  Superstore.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BgzamZif_Z7--YlSvXcPyaXZUJfOkQyH
"""

# ======================================================
# Retail Sales EDA (Sample Superstore)
# Author: ChatGPT
# Purpose: Resume-quality, step-by-step EDA pipeline (cleaning → features → EDA → stats → insights)
# Requirements: pandas, numpy, matplotlib, seaborn
# Datafile expected: SampleSuperstore.csv
# ======================================================

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ---------------- 0. CONFIG ----------------
sns.set_style("whitegrid")
pd.set_option("display.max_columns", None)
OUTPUT_DIR = "eda_outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def save_fig(fname):
    path = os.path.join(OUTPUT_DIR, fname)
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    print(f"Saved figure: {path}")

# ---------------- 1. LOAD DATA ----------------
file_name = "SampleSuperstore.csv"
df = pd.read_csv(file_name)

# 1.1 Normalize column names (lowercase, non-alnum -> underscore)
df.columns = (df.columns
                .astype(str)
                .str.strip()
                .str.lower()
                .str.replace('[^0-9a-z]+', '_', regex=True)
                .str.strip('_'))

print("\n1) Initial data loaded.")
print(" - Shape:", df.shape)
print(" - Columns:", df.columns.tolist())

# ---------------- 2. INITIAL INSPECTION ----------------
print("\n2) Quick view:")
print(df.head(3).T)
print("\n2.1 Data types:")
print(df.dtypes)
print("\n2.2 Missing value counts:")
print(df.isnull().sum())

# ---------------- 3. CLEANING & TYPE CONVERSION ----------------
# 3.1 Drop exact duplicates
dupes = df.duplicated().sum()
if dupes:
    print(f"\n3.1 Dropping {dupes} duplicate rows.")
    df = df.drop_duplicates()

# 3.2 Date parsing (if present)
for date_col in ["order_date", "ship_date"]:
    if date_col in df.columns:
        df[date_col] = pd.to_datetime(df[date_col], errors="coerce")

# 3.3 Numeric conversion for expected columns
numeric_cols = ["sales", "profit", "quantity", "discount"]
for c in numeric_cols:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")

# 3.4 Fill / impute small missing values (median for numeric, Unknown for categorical)
num_missing = df.select_dtypes(include=[np.number]).isnull().sum()
cat_missing = df.select_dtypes(include=["object"]).isnull().sum()

# Fill numeric with median (documented decision)
filled = {}
for c in num_missing[num_missing > 0].index:
    med = df[c].median()
    df[c] = df[c].fillna(med)
    filled[c] = med

# Fill categorical with 'Unknown'
for c in cat_missing[cat_missing > 0].index:
    df[c] = df[c].fillna("Unknown")

if filled:
    print("\n3.4 Numeric columns filled with medians:", filled)

# ---------------- 4. FEATURE ENGINEERING ----------------
# 4.1 Time features
if "order_date" in df.columns:
    df["year"] = df["order_date"].dt.year
    df["month"] = df["order_date"].dt.month
    df["order_ym"] = df["order_date"].dt.to_period("M").astype(str)
    df["weekday"] = df["order_date"].dt.day_name()
    df["quarter"] = df["order_date"].dt.to_period("Q").astype(str)

# 4.2 Profit margin and revenue per unit
if {"sales", "profit", "quantity"}.issubset(df.columns):
    df["profit_margin"] = np.where(df["sales"] == 0, np.nan, df["profit"] / df["sales"])
    df["revenue_per_unit"] = df["sales"] / df["quantity"].replace({0: np.nan})

# 4.3 Log transforms to help with skew (safe)
if "sales" in df.columns:
    df["log1p_sales"] = np.log1p(df["sales"].clip(lower=0))
if "profit" in df.columns:
    # shift to make non-negative for log1p if necessary
    shift = 0
    min_profit = df["profit"].min() if "profit" in df.columns else 0
    if min_profit < 0:
        shift = -min_profit
    df["log1p_profit"] = np.log1p(df["profit"] + shift)

# 4.4 Discount bucket (ordered)
if "discount" in df.columns:
    bins = [-0.01, 0.05, 0.1, 0.25, 0.5, 1.0]
    labels = ["no_disc", "tiny", "small", "medium", "high"]
    df["discount_bucket"] = pd.cut(df["discount"], bins=bins, labels=labels, include_lowest=True, ordered=True)

print("\n4) Feature engineering completed. Some new columns:", [c for c in ["year","month","order_ym","weekday","quarter","profit_margin","revenue_per_unit","log1p_sales","log1p_profit","discount_bucket"] if c in df.columns])

# ---------------- 5. SAVE CLEANED SNAPSHOT ----------------
clean_sample = df.sample(min(2000, len(df)), random_state=42)
clean_sample.to_csv(os.path.join(OUTPUT_DIR, "cleaned_sample.csv"), index=False)
print(f"\n5) Saved cleaned sample to {os.path.join(OUTPUT_DIR, 'cleaned_sample.csv')}")

# ---------------- 6. EXPLORATORY OVERVIEW ----------------
print("\n6) Exploratory overview (summary stats):")
print(df.describe(include="all").T)

# Univariate: display histograms for important numeric columns
plot_hist_cols = [c for c in ["sales", "profit", "profit_margin", "quantity", "revenue_per_unit"] if c in df.columns]
for c in plot_hist_cols:
    plt.figure(figsize=(8,4))
    sns.histplot(df[c].dropna(), bins=50, kde=True)
    plt.title(f"Distribution of {c}")
    save_fig(f"hist_{c}.png")
    plt.show()

# ---------------- 7. IQR & OUTLIER DETECTION ----------------
print("\n7) IQR-based outlier detection (sales, profit, profit_margin)")
outlier_info = {}
for c in ["sales", "profit", "profit_margin"]:
    if c in df.columns:
        q1 = df[c].quantile(0.25)
        q3 = df[c].quantile(0.75)
        iqr = q3 - q1
        lower = q1 - 1.5 * iqr
        upper = q3 + 1.5 * iqr
        mask = (df[c] < lower) | (df[c] > upper)
        count = mask.sum()
        outlier_info[c] = {"q1": q1, "q3": q3, "iqr": iqr, "lower": lower, "upper": upper, "count": int(count)}
        print(f" - {c}: Q1={q1:.2f}, Q3={q3:.2f}, IQR={iqr:.2f}, lower={lower:.2f}, upper={upper:.2f}, outliers={count}")
        # save outlier rows sample
        sample_out = df.loc[mask, df.columns.intersection(["order_id","order_date","category","sub_category","product_name", c])].head(10)
        if not sample_out.empty:
            sample_out.to_csv(os.path.join(OUTPUT_DIR, f"outliers_sample_{c}.csv"), index=False)
            print(f"   -> Saved sample outliers for {c} to {os.path.join(OUTPUT_DIR, f'outliers_sample_{c}.csv')}")
# Save outlier info summary
pd.DataFrame.from_dict(outlier_info, orient="index").to_csv(os.path.join(OUTPUT_DIR, "outlier_summary.csv"))
print(f"Saved outlier_summary.csv in {OUTPUT_DIR}")

# ---------------- 8. BOXPLOTS (showing outliers) ----------------
print("\n8) Boxplots (showing outliers)")

# Sales by Category (showfliers=True => outliers visible)
if {"category", "sales"}.issubset(df.columns):
    plt.figure(figsize=(9,6))
    sns.boxplot(x="category", y="sales", data=df, showfliers=True)
    plt.title("Sales by Category (boxplot w/ outliers)")
    save_fig("box_sales_by_category.png")
    plt.show()

# Profit by Region
if {"region", "profit"}.issubset(df.columns):
    plt.figure(figsize=(9,6))
    sns.boxplot(x="region", y="profit", data=df, showfliers=True)
    plt.title("Profit by Region (boxplot w/ outliers)")
    save_fig("box_profit_by_region.png")
    plt.show()

# Revenue per Unit by top-10 sub_category (horizontal)
if {"sub_category", "revenue_per_unit"}.issubset(df.columns):
    rp = df[["sub_category", "revenue_per_unit"]].dropna()
    top_subs = rp.groupby("sub_category")["revenue_per_unit"].median().sort_values(ascending=False).head(10).index
    plt.figure(figsize=(10,6))
    sns.boxplot(x="revenue_per_unit", y="sub_category", data=rp[rp["sub_category"].isin(top_subs)], order=top_subs, showfliers=True)
    plt.title("Revenue per Unit by Top-10 Sub-Categories (boxplot)")
    save_fig("box_revenue_unit_top10_subcats.png")
    plt.show()

# ---------------- 9. BIVARIATE & MULTIVARIATE ----------------
print("\n9) Bivariate analysis (correlations, scatter-plots, categorical aggregates)")

# 9.1 Correlation heatmap for numeric variables
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
if len(num_cols) >= 2:
    plt.figure(figsize=(10,7))
    corr = df[num_cols].corr()
    sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm")
    plt.title("Correlation heatmap (numerical)")
    save_fig("corr_heatmap.png")
    plt.show()

# 9.2 Discount vs Profit scatter
if {"discount", "profit"}.issubset(df.columns):
    plt.figure(figsize=(8,5))
    sns.scatterplot(x="discount", y="profit", data=df, alpha=0.4)
    plt.title("Discount vs Profit (scatter)")
    save_fig("scatter_discount_profit.png")
    plt.show()

# 9.3 Sales vs Profit scatter (identifies heavy orders / loss makers)
if {"sales", "profit"}.issubset(df.columns):
    plt.figure(figsize=(8,5))
    sns.scatterplot(x="sales", y="profit", data=df, alpha=0.4)
    plt.title("Sales vs Profit")
    save_fig("scatter_sales_profit.png")
    plt.show()

# 9.4 Category and Segment summaries
if "category" in df.columns:
    cat_perf = df.groupby("category").agg(total_sales=("sales","sum"), total_profit=("profit","sum"), avg_margin=("profit_margin","mean")).sort_values("total_sales", ascending=False)
    cat_perf.to_csv(os.path.join(OUTPUT_DIR, "category_performance.csv"))
    print(" - Saved category_performance.csv")
    display = cat_perf.head(10)
    print("\nTop categories by sales:\n", display)

if "segment" in df.columns:
    seg_perf = df.groupby("segment").agg(total_sales=("sales","sum"), total_profit=("profit","sum"), avg_margin=("profit_margin","mean")).sort_values("total_sales", ascending=False)
    seg_perf.to_csv(os.path.join(OUTPUT_DIR, "segment_performance.csv"))
    print(" - Saved segment_performance.csv")
    print("\nSegment performance:\n", seg_perf)

# 9.5 Top states and cities by sales/profit
if "state" in df.columns:
    state_perf = df.groupby("state").agg(total_sales=("sales","sum"), total_profit=("profit","sum")).sort_values("total_sales", ascending=False)
    state_perf.head(15).to_csv(os.path.join(OUTPUT_DIR, "top_states_by_sales.csv"))
    print(" - Saved top_states_by_sales.csv")

if "city" in df.columns:
    city_perf = df.groupby("city").agg(total_sales=("sales","sum")).sort_values("total_sales", ascending=False)
    city_perf.head(15).to_csv(os.path.join(OUTPUT_DIR, "top_cities_by_sales.csv"))
    print(" - Saved top_cities_by_sales.csv")

# ---------------- 10. TIME-SERIES TRENDS ----------------
print("\n10) Time-series trends (monthly sales, seasonality)")

if "order_ym" in df.columns:
    monthly_sales = df.groupby("order_ym")["sales"].sum()
    plt.figure(figsize=(12,5))
    monthly_sales.plot(marker="o")
    plt.title("Monthly Sales Trend")
    plt.ylabel("Sales")
    save_fig("monthly_sales_trend.png")
    plt.show()

    # rolling mean (3-month)
    monthly_sales_rolling = monthly_sales.rolling(3, center=False).mean()
    plt.figure(figsize=(12,4))
    monthly_sales.plot(alpha=0.5)
    monthly_sales_rolling.plot()
    plt.title("Monthly Sales + 3-month Rolling Average")
    save_fig("monthly_sales_rolling.png")
    plt.show()

# Weekday order counts
if "weekday" in df.columns:
    weekday_counts = df["weekday"].value_counts().reindex(["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"])
    plt.figure(figsize=(9,4))
    sns.barplot(x=weekday_counts.index, y=weekday_counts.values, palette="Set2")
    plt.title("Orders by Weekday")
    plt.xticks(rotation=10)
    save_fig("orders_by_weekday.png")
    plt.show()

# ---------------- 11. PARETO (80/20) ON PRODUCTS ----------------
print("\n11) Pareto (top products contribution)")

if "product_name" in df.columns:
    prod_sales = df.groupby("product_name")["sales"].sum().sort_values(ascending=False)
    cum = prod_sales.cumsum() / prod_sales.sum()
    pareto_df = pd.DataFrame({"sales": prod_sales, "cum_pct": cum})
    pareto_df.head(10).to_csv(os.path.join(OUTPUT_DIR, "top_products_pareto.csv"))
    plt.figure(figsize=(10,4))
    plt.plot(pareto_df["cum_pct"].iloc[:200].values)
    plt.axhline(0.8, color="red", linestyle="--")
    plt.title("Pareto: Cumulative product sales (top 200 shown)")
    save_fig("pareto_products.png")
    plt.show()
    print(" - Saved top_products_pareto.csv")

# ---------------- 12. PROBABILITY MODELS ----------------
print("\n12) Probability approximations (Normal, Binomial, Poisson)")

# 12.1 Normal fit overlay on log1p_sales (more Gaussian after transform)
if "log1p_sales" in df.columns:
    vals = df["log1p_sales"].dropna()
    mu, sigma = vals.mean(), vals.std()
    plt.figure(figsize=(8,4))
    sns.histplot(vals, bins=40, kde=False, stat="density", color="skyblue")
    x = np.linspace(vals.min(), vals.max(), 200)
    normal_pdf = (1/(sigma * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x - mu)/sigma)**2)
    plt.plot(x, normal_pdf, color="red", label=f"Normal PDF (μ={mu:.2f},σ={sigma:.2f})")
    plt.title("Normal Approximation on log1p(Sales)")
    plt.legend()
    save_fig("normal_fit_log1p_sales.png")
    plt.show()

# 12.2 Binomial: probability of profitable order
if "profitable_flag" not in df.columns and "profit" in df.columns:
    df["profitable_flag"] = (df["profit"] > 0).astype(int)
if "profitable_flag" in df.columns:
    p_hat = df["profitable_flag"].mean()
    print(f" - Binomial: observed p̂ (probability order is profitable) = {p_hat:.3f}")
    plt.figure(figsize=(5,4))
    sns.countplot(x="profitable_flag", data=df, palette="Set2")
    plt.xticks([0,1], ["Loss","Profit"])
    plt.title("Counts: Loss vs Profit Orders")
    save_fig("binomial_profit_counts.png")
    plt.show()

# 12.3 Poisson: orders per day
if "order_date" in df.columns:
    orders_per_day = df.groupby("order_date").size()
    lam = orders_per_day.mean()
    print(f" - Poisson approx: λ = mean orders/day = {lam:.2f}")
    plt.figure(figsize=(8,4))
    sns.histplot(orders_per_day, bins=30, kde=False, color="green")
    plt.title("Distribution of Orders per Day")
    save_fig("poisson_orders_per_day.png")
    plt.show()

# ---------------- 13. HYPOTHESIS TESTING: MANUAL ANOVA ----------------
print("\n13) Hypothesis testing: Manual ANOVA (profit by region)")

if {"region","profit"}.issubset(df.columns):
    regions = df["region"].dropna().unique().tolist()
    groups = [df.loc[df["region"]==r,"profit"].dropna().values for r in regions]
    groups = [g for g in groups if len(g) > 1]
    if len(groups) >= 2:
        all_vals = np.concatenate(groups)
        grand_mean = all_vals.mean()
        ss_between = sum(len(g) * (g.mean() - grand_mean)**2 for g in groups)
        ss_within = sum(((g - g.mean())**2).sum() for g in groups)
        df_between = len(groups) - 1
        df_within = len(all_vals) - len(groups)
        ms_between = ss_between / df_between if df_between>0 else np.nan
        ms_within = ss_within / df_within if df_within>0 else np.nan
        F = ms_between / ms_within if (ms_within and ms_within>0) else np.nan
        eta2 = ss_between / (ss_between + ss_within) if (ss_between + ss_within)>0 else np.nan
        print(f" - ANOVA manual F-statistic = {F:.3f}, eta-squared = {eta2:.3f}")
        print("   (Note: p-value not computed because scipy/statsmodels not used here; interpret F and effect size.)")
    else:
        print(" - Not enough groups/samples to perform ANOVA.")

# ---------------- 14. ACTIONABLE INSIGHTS & SAVE ----------------
print("\n14) Actionable insights (automatic suggestions):")

insights = []

# A. Pareto: top categories or products
if "category" in df.columns:
    top_cat = df.groupby("category")["sales"].sum().sort_values(ascending=False)
    top_cat_pct = top_cat.cumsum() / top_cat.sum()
    insights.append(f"Top category by sales: {top_cat.index[0]} (contributes {top_cat.iloc[0]/top_cat.sum():.1%} of sales)")

# B. Discount impact
if {"discount","profit_margin"}.issubset(df.columns):
    disc_impact = df.groupby(pd.cut(df["discount"], [-0.01,0.05,0.1,0.25,0.5,1.0]))["profit_margin"].mean()
    insights.append("Discount buckets show declining profit margin as discount increases (recommend targeted discounts).")
    disc_impact.to_csv(os.path.join(OUTPUT_DIR,"discount_profit_margin_by_bucket.csv"))

# C. Loss-making sub-categories
if "sub_category" in df.columns:
    loss_subs = df.groupby("sub_category")["profit"].sum().sort_values().head(10)
    loss_subs.to_csv(os.path.join(OUTPUT_DIR, "loss_making_subcategories.csv"))
    insights.append("Saved top loss-making sub-categories for review: loss_making_subcategories.csv")

# D. Shipping delays (if present)
if "ship_delay_days" in df.columns:
    med_delay = df["ship_delay_days"].median()
    insights.append(f"Median shipping delay = {med_delay} days; consider logistics optimization if > 2 days.")

# Print and save insights
for i, it in enumerate(insights, 1):
    print(f"  {i}. {it}")
with open(os.path.join(OUTPUT_DIR, "insights_summary.txt"), "w") as f:
    f.write("\n".join([f"{i}. {it}" for i,it in enumerate(insights,1)]))
print(f"\nSaved insights_summary.txt to {OUTPUT_DIR}")

# ---------------- 15. FINAL NOTES ----------------
print("\n15) EDA run complete. Key outputs saved to folder:", OUTPUT_DIR)
print(" - cleaned_sample.csv, category_performance.csv, segment_performance.csv, top_states_by_sales.csv,")
print(" - outlier_summary.csv, outliers_sample_*.csv, pareto_products.png, monthly_sales_trend.png, etc.")
print("\nUse the saved CSVs and PNGs for your GitHub README and project write-up.")